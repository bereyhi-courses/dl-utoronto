---
layout: default
---

<div class="home">
  
  <h1>{{ site.course_name }} <span style="font-size: 20px;color: #0006;">/ {{ site.course_semester }} </span></h1>
  
  {% include announcements.html %}
  <br />
  
  {{ content }}

  <h2>Course Description</h2>
  <p>{{ site.course_description }}</p>
  
  

  <h2>Time and Place</h2>
  <h3>Lectures</h3>
  <p>Lectures start on <strong>{{ site.start_date }}</strong>. {{site.start_description}}</p>
  <table style="border-collapse: collapse; width: 90%;margin-top: 10pt; margin-bottom: 10pt; border: .5px solid black;">
    <tr class="row1" style="background-color: rgb(242, 242, 242);">
      <td><em>Day</em></td>
      <td><em>Time</em></td>
      <td><em>Place</em></td>
    </tr>
    <tr class="row2" style="background-color: white;">
      <td><strong>{{ site.day1 }}</strong></td>
      <td><strong>{{ site.time1 }}</strong></td>
      <td>{{ site.loc1 }}</td>
    </tr>
    <tr class="row3" style="background-color: white;">
      <td><strong>{{ site.day2 }}</strong></td>
      <td><strong>{{ site.time2 }}</strong></td>
      <td>{{ site.loc2 }}</td>
    </tr>
  </table>


  <h3>Tutorials</h3>
  <p>Tutorials sessions start on <strong>{{ site.start_dateTut }}</strong>.</p>
  <table style="border-collapse: collapse; width: 90%;margin-top: 10pt; margin-bottom: 10pt; border: .5px solid black;">
    <tr class="row1" style="background-color: rgb(242, 242, 242);">
      <td><em>Day</em></td>
      <td><em>Time</em></td>
      <td><em>Place</em></td>
    </tr>
    <tr class="row2" style="background-color: white;">
      <td><strong>{{ site.dayTut }}</strong></td>
      <td><strong>{{ site.timeTut }}</strong></td>
      <td>{{ site.locTut }}</td>
    </tr>
  </table>

   <h3>Course Office Hours</h3>
  
   <table style="border-collapse: collapse; width: 90%;margin-top: 10pt; margin-bottom: 10pt; border: .5px solid black;">
    <tr class="row1" style="background-color: rgb(242, 242, 242);">
      <td><em>Day</em></td>
      <td><em>Time</em></td>
    </tr>
    <tr class="row2" style="background-color: white;">
      <td><strong>{{ site.dayOffice }}</strong></td>
      <td><strong>{{ site.timeOffice }}</strong></td>
    </tr>
  </table>
  
  {% if site.data.previous_offering and site.data.previous_offering.offerings and site.data.previous_offering.offerings.size > 0 %}
  <h2>Previous Offerings</h2>
  <ul class="previous-offerings">
    {% for offer in site.data.previous_offering.offerings %}
    <li><a href="{{ offer.url }}">{{offer.title}}</a></li>
    {% endfor %}
  </ul>
  {% endif %}
  
  <br>
  <!-- <div> -->
    <div style="width:95%; float: left">
      <!-- <h2>Instructors</h2> -->
      <!-- <div class="image--cover-container">
        <img src="{{site.data.people.instructor.profile_pic | prepend: site.baseurl }}" class="image--cover">
        <p>{{site.data.people.instructor.name}}</p>
      </div> -->
      
      <div class="profile-pic-gallary" style="width:95%; float: left">
        <h2>Instructor</h2>
        {% for ins in site.data.people.instructors %}
        <div class="image--cover-container">
          <img src="{{ins.profile_pic | prepend: site.baseurl }}" class="image--cover">
          {% if ins.webpage %}
          <p><a href="{{ ins.webpage }}">{{ins.name}}</a></p>
          {% else %}
          <p>{{ins.name}}</p>
          {% endif %}
        </div>
        <p style="margin: 0;">{{ins.title}}</p>
        <p style="margin: 0;">{{ins.affil}}</p>   
        <p style="margin: 0;">{{ins.address}}</p>
        {% endfor %}
      </div>
    </div>
    
    <div style="width:95%; float: left">
      <div class="profile-pic-gallary ">
        <h2>Teaching Assistants</h2>
        {% for ta in site.data.people.teaching_assistants %}
        <div class="image--cover-container" style="width:20%; float: left">
          <img src="{{ta.profile_pic | prepend: site.baseurl }}" class="image--cover">
          {% if ta.webpage %}
          <p><a href="{{ ta.webpage }}">{{ta.name}}</a></p>
          {% else %}
          <p>{{ta.name}}</p>
          {% endif %}
        </div>
        {% endfor %}
      </div>
    </div>
  <!-- </div> -->
</div>

<h2>Course Syllabus</h2>

<p>The course is given in three parts, namely <em>Fundamentals of Computational Learning</em>, <em>Deep Neural Networks</em>, and <em>Advances in Deep Learning</em>. The contents taught in each part are listed below:</p>
<p>
<div style="background-color: rgba(244, 233, 233, 0.99); padding: 10px;">
<h3>Part I: Principles of Computational Learning</h3>
<ol>
  <li><strong>Learning from Data</strong></li>
  <ul>
    <li>Data-driven Approaches</li>
    <li>Concept of Learning</li>
  </ul>
  <li><strong>Basic Definitions</strong></li>
  <ul>
    <li>Types of Learning Tasks</li>
    <li>Key Components of Computational Learning</li>
    <ul>
      <li>Data</li>
      <li>Model</li>
      <li>Loss</li>
    </ul>
    <li>Neural Networks</li>
    <ul>
      <li>Basic Computational Unit: <em>Artificial Neuron</em></li>
      <li>Perceptron Machine</li>
      <li>Artificial Neural Networks</li>
      <li>Deep Neural Networks</li>
    </ul>
  </ul>
  <li><strong>Learning via Deep Models</strong></li>
  <ul>
    <li>Universal Approximation Theorem</li>
    <li>Training Deep Models via Empirical Risk Minimization</li>
    <li>Inference via Deep Models</li>
    <li>Building Training Loop via Gradient Descent Algorithm</li>
    <ul>
      <li>Review: Function Optimization</li>
      <li>Review: Gradient Descent Algorithm</li>
    </ul>
  </ul>
</ol>
</div>

<div style="background-color: rgba(246, 250, 246, 0.9); padding: 10px;">
<h3>Part II: Deep Neural Networks</h3>
<ol>
  <li><strong>Fully-connected Feedforward Neural Networks</strong></li>
  <ul>
    <li>Deep Multilayer Perceptrons (MLPs)</li>
    <li>Inference and Training of MLPs</li>
    <ul>
      <li>Inference via Forward Propagation</li>
      <li>Training by Sample Gradients</li>
      <li>Challenge of Computing Gradient</li>
    </ul>
    <li>Backpropagation Algorithm</li>
    <ul>
      <li>Computation Graph</li>
      <li>Computing Gradients on Graphs via Forward and Backward Pass</li>
      <li>Backward Pass on Basic Vector Operations</li>
      <li>Backpropagating over MLPs</li>
      <li>Duality of Forward and Backward Computations in MLPs</li>
    </ul>
    <li>Building Training Loop for MLPs</li>
    <ul>
      <li>Mini-Batch Stochastic Gradient Descent (SGD)</li>
      <li>Complexity-Variance Trade-off in SGD</li>
    </ul>
    <li>Evaluating Trained Models</li>
    <ul>
      <li>Independence of Test and Training Samples</li>
      <li>Learning Curves and Classic Evaluation Metrics</li>
    </ul>
    <li>Examples: Classification and Regression by MLPs</li>
  </ul>

  <li><strong>Advances in Optimization, Model Fitting and Data Preprocessing</strong></li>
  <ul>
    <li>Gradient-based Optimizers</li>
    <ul>
      <li>Stabilizing SGD via Learning-Rate Scheduling</li>
      <li>Momentum and Its Bias-Variance Trade-off</li>
      <li>Dimension-level Learning-Rate Scheduling: Resilient Backpropagation (Rprop)</li>
      <li>Dimension-level Scheduling by Momentum: Root Mean Square Propagation (RMSprop)</li>
      <li>Adaptive Moment Estimation (Adam) Algorithm</li>
    </ul>
    <li>Overfitting and Underfitting</li>
    <ul>
      <li>Concept of Over- and Underfitting</li>
      <li>Roots of Overfitting: Model Complexity, Data Sufficiency and Co-adaptation</li>
      <li>Dealing with Overfitting I: Hyperparameter Tuning over Validation Dataset</li>
      <li>Dealing with Overfitting II: Regularization and Dropout</li>
    </ul>
    <li>Statistical Viewpoint on Data</li>
    <ul>
      <li>Data Space and Data Distribution</li>
      <li>Dealing with Overfitting III: Data Augmentation and Synthetic Data Generation</li>
      <li>Basics of Data Cleaning</li>
    </ul>
    <li>Standardization and Normalization</li>
    <ul>
      <li>Input Normalization</li>
      <li>Batch Normalization</li>
      <li>Layer Normalization</li>
    </ul>
  </ul>

  <li><strong>Convolutional Neural Networks (CNNs)</strong></li>
  <ul>
    <li>Locally-Connected Models with Shared Weights</li>
    <ul>
      <li>Local Connectivity versus Full Connectivity</li>
      <li>Computational Models with Shared Parameters</li>
    </ul>
    <li>Components of CNNs</li>
    <ul>
      <li>Tensor-type Convolution and Convolutional Layer</li>
      <li>Pooling Layer</li>
      <li>Upsampling and Downsampling Layers</li>
    </ul>
    <li>Building a Custom CNN</li>
    <ul>
      <li>Deep CNN for Image Processing</li>
      <li>Resizing and Normalization</li>
      <li>Receptive Field and Feature Extraction</li>
    </ul>
    <li>Training CNNs</li>
    <ul>
      <li>Backpropagation through Convolutional Layer</li>
      <li>Backpropagation through Pooling Layer</li>
      <li>Backpropagation through Resampling Layers</li>
    </ul>
    <li>Examples of Deep CNNs</li>
  </ul>

  <li><strong>Residual Learning</strong></li>
  <ul>
    <li>ILSVR Challenge: ImageNet</li>
    <li>Depth Challenge</li>
    <ul>
      <li>Vanishing Gradient</li>
      <li>Exploding Gradient</li>
    </ul>
    <li>Residual Learning</li>
    <ul>
      <li>Learning Residual Function</li>
      <li>Skip Connection</li>
      <li>Concentration Property of Gradient in Residual Learning</li>
    </ul>
    <li>Residual Networks (ResNets)</li>
    <ul>
      <li>ResNet Units</li>
      <li>Deep ResNets</li>
      <li>Short and Long Skip Connections</li>
    </ul>
    <li>State-of-the-Art Residual Architectures</li>
  </ul>

  <li><strong>Recurrent Neural Networks (RNNs)</strong></li>
  <ul>
    <li>Sequence Learning by Recurrence</li>
    <ul>
      <li>Sequential Data and State-Space Models</li>
      <li>Building Computational State-Space Models by Recurrence</li>
      <li>Elman and Jordan Networks</li>
      <li>Forward Propagation Through Time</li>
      <li>Deep Recurrent Neural Networks (RNNs)</li>
    </ul>
    <li>Training RNNs</li>
    <ul>
      <li>Backpropagation Through Time (BPTT)</li>
      <li>Vanishing and Exploding Gradient in BPTT</li>
      <li>Truncated BPTT</li>
    </ul>
    <li>Concept of Gating</li>
    <ul>
      <li>Notion of Gate and Gating Mechanism</li>
      <li>Building a Computational Gate</li>
      <li>Gated Architecture I: Gated Recurrent Unit (GRU)</li>
      <li>Gated Architecture II: Long Short-Term Memory (LSTM)</li>
    </ul>
    <li>Bidirectional Sequence Processing</li>
    <li>Segmentation in Sequence Learning</li>
    <ul>
      <li>Pre-segmentation of Data versus Labeling Unsegmented Data</li>
      <li>Connectionist Temporal Classification (CTC) Method</li>
    </ul>
  </ul>
</ol>
</div>

<div style="background-color: rgba(221, 236, 239, 0.99); padding: 10px;">
<h3>Part III: Advances in Deep Learning</h3>
<ol>
  <li><strong>Sequence-to-Sequence Models</strong></li>
  <ul>
    <li>Encoding and Decoding via Computational Models</li>
    <ul>
      <li>General Learning Tasks with Sequence Data</li>
      <li>Learning Universal Context</li>
    </ul>
    <li>Basic Language Model</li>
    <ul>
      <li>Processing text to tokens</li>
      <li>Learning to predict next tokens</li>
    </ul>
    <li>Neural Machine Translation (NMT)</li>
    <ul>
      <li>Basic Translator by RNNs</li>
      <li>Restricted Memory of Recurrent NMT</li>
    </ul>
    <li>Attention Mechanism and Transformer</li>
    <ul>
      <li>Attention Mechanism</li>
      <li>Computational Modeling of Attention: Key and Query Correlation</li>
      <li>Using Attention for Context Extraction: Cross-Attention</li>
      <li>Sequence Processing with Attention</li>
      <li>Multihead Self-Attention</li>
      <li>Transformer: Encoding-Decoding via Attention</li>
    </ul>
  </ul>

  <li><strong>Deep Unsupervised Learning</strong></li>
  <ul>
    <li>Review: Principle Component Analysis (PCA)</li>
    <li>Autoencoding</li>
    <ul>
      <li>Nonlinear PCA by Computational Models</li>
      <li>Notion of Latent Representation</li>
      <li>Autoencoders</li>
    </ul>
    <li>Training Autoencoders</li>
    <ul>
      <li>Training Vanilla Autoencoder</li>
      <li>Training Denoising Autoencoder</li>
      <li>Constraining Latent Representation by Regularization</li>
    </ul>
    <li>Data Generation by Autoencoders</li>
    <ul>
      <li>Naive Generative Model by Vanilla Autoencoder</li>
      <li>Variational Autoencoders (VAE)</li>
      <li>Training and Sampling VAEs</li>
    </ul>
  </ul>
</ol>
</div>
</p>


<h2>Course Evaluation</h2>
<p>The learning procedure consists of three components:
  <table style="border-collapse: collapse; width: 90%;margin-top: 10pt; margin-bottom: 10pt;">
    <tr class="row1" style="background-color: rgb(242, 242, 242);">
      <td><em>Component</em></td>
      <td><em>Grade %</em></td>
      <td><em></em></td>
    </tr>
    <tr class="row2" style="background-color: rgb(201, 235, 247);">
      <td><strong>Assignments</strong></td>
      <td><strong>{{site.assgn_mark}}</strong></td>
      <td><em>{{site.assgn_desc}}</em></td>
    </tr>
    <tr class="row3" style="background-color: rgb(217, 244, 217);">
      <td><strong>Exam</strong></td>
      <td><strong>{{site.exam_mark}}</strong></td>
      <td><em>{{site.exam_desc}}</em></td>
    </tr>
    <tr class="row4" style="background-color: rgb(252, 215, 215);">
      <td><strong>Project</strong></td>
      <td><strong>{{site.project_mark}}</strong></td>
      <td><em>{{site.project_desc}}</em></td>
    </tr>
  </table>
  Let's get through them a bit in detail.</p>
  
  <h2>Assignment</h2>
  <p>{{site.assgn_txt}}</p>
  
  <h2>Exam</h2>
  <p>{{site.exam_txt}}</p>
  
  <h2>Course Project</h2>
  <p>{{site.project_txt}}</p>
  