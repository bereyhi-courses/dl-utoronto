---
type: lecture
date: 2025-09-02T12:40:00
title: "Lecture 2: ML Component 1 - Data"
tldr: "ML Part II - Data"
stat: lec
# for lectures stat: lec
description: We saw that each ML solution contains 3 components of Data, Model and Loss. We now get to know Data.
videoID: zHY6KZfqE6g 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 1 - Section 2]({{ site.baseurl }}/assets/Notes/CH1/CH1_Sec2.pdf) 

<!-- **Further Reads:**
* [Tokenization](https://web.stanford.edu/~jurafsky/slp3/2.pdf): Chapter 2 of [[JM]](https://web.stanford.edu/~jurafsky/slp3/)
* [Embedding](https://web.stanford.edu/~jurafsky/slp3/6.pdf): Chapter 6 of [[JM]](https://web.stanford.edu/~jurafsky/slp3/)
* [Original BPE Algorithm](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM): Original BPE Algorithm proposed by Philip Gage in 1994
* [BPE for Tokenization](https://arxiv.org/abs/1508.07909): Paper _Neural machine translation of rare words with subword units_ by _Rico Sennrich, Barry Haddow, and Alexandra Birch_ presented in ACL 2016 that adapted BPE for NLP -->